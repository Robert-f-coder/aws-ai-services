{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face collections using Amazon Rekognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "This notebook provides a walkthrough of [object detection API](https://docs.aws.amazon.com/rekognition/latest/dg/labels.html) in Amazon Rekognition to identify objects.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rx/cymhggws62j1jghcp9jg6yj00000gs/T/ipykernel_65680/3630156363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mIImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "# Initialise Notebook\n",
    "import boto3\n",
    "from IPython.display import HTML, display, Image as IImage\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "# Curent AWS Region. Use this to choose corresponding S3 bucket with sample content\n",
    "\n",
    "mySession = boto3.session.Session()\n",
    "awsRegion = mySession.region_name\n",
    "print(awsRegion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init clients\n",
    "rekognition = boto3.client('rekognition')\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket that contains sample images and videos\n",
    "\n",
    "# We are providing sample images and videos in this bucket so\n",
    "# you do not have to manually download/upload test images and videos.\n",
    "\n",
    "bucketName = \"m2c-gps304-workshop-\" + awsRegion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary directory\n",
    "# This directory is not needed to call Rekognition APIs.\n",
    "# We will only use this directory to download images from S3 bucket and draw bounding boxes\n",
    "\n",
    "!mkdir m1tmp\n",
    "tempFolder = 'm1tmp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a face collection\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"BuildEnterpriseAIApps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection:BuildEnterpriseAIApps\n",
      "Collection ARN: aws:rekognition:us-west-2:185818737892:collection/BuildEnterpriseAIApps\n",
      "Status code: 200\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "print('Creating collection:' + collection_id)\n",
    "response=rekognition.create_collection(CollectionId=collection_id)\n",
    "print('Collection ARN: ' + response['CollectionArn'])\n",
    "print('Done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Rekognition to detect objects in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidS3ObjectException",
     "evalue": "An error occurred (InvalidS3ObjectException) when calling the IndexFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidS3ObjectException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rx/cymhggws62j1jghcp9jg6yj00000gs/T/ipykernel_65680/643701394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfacecollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facecollection1.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m faceIndexResponse=rekognition.index_faces(CollectionId=collection_id,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                 \u001b[0mImage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'S3Object'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Bucket'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbucketName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfacecollection\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 \u001b[0mExternalImageId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecollection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CodeBase/aws-service-examples/amazon-rekognition-raver/venv/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    387\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CodeBase/aws-service-examples/amazon-rekognition-raver/venv/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidS3ObjectException\u001b[0m: An error occurred (InvalidS3ObjectException) when calling the IndexFaces operation: Unable to get object metadata from S3. Check object key, region and/or access permissions."
     ]
    }
   ],
   "source": [
    "# Call Amazon Rekognition to index faces in the image\n",
    "# https://docs.aws.amazon.com/rekognition/latest/dg/API_DetectLabels.html\n",
    "\n",
    "\n",
    "facecollection = \"facecollection1.jpg\"\n",
    "faceIndexResponse=rekognition.index_faces(CollectionId=collection_id,\n",
    "                                Image={'S3Object':{'Bucket':bucketName,'Name':facecollection}},\n",
    "                                ExternalImageId=facecollection,\n",
    "                                MaxFaces=1,\n",
    "                                QualityFilter=\"AUTO\",\n",
    "                                DetectionAttributes=['ALL'])\n",
    "\n",
    "print ('Results for ' + facecollection) \t\n",
    "print('Faces indexed:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the raw JSON reponse from Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FaceRecords': [{'Face': {'FaceId': '50d567f5-a820-4ad7-b478-f835fb599ff5',\n",
       "    'BoundingBox': {'Width': 0.11128479242324829,\n",
       "     'Height': 0.22814197838306427,\n",
       "     'Left': 0.46769365668296814,\n",
       "     'Top': 0.20815825462341309},\n",
       "    'ImageId': '805a5bb9-6f62-3e3c-89a9-96fde1cde3c7',\n",
       "    'ExternalImageId': 'facecollection1.jpg',\n",
       "    'Confidence': 99.99779510498047},\n",
       "   'FaceDetail': {'BoundingBox': {'Width': 0.11128479242324829,\n",
       "     'Height': 0.22814197838306427,\n",
       "     'Left': 0.46769365668296814,\n",
       "     'Top': 0.20815825462341309},\n",
       "    'AgeRange': {'Low': 22, 'High': 34},\n",
       "    'Smile': {'Value': True, 'Confidence': 99.6400375366211},\n",
       "    'Eyeglasses': {'Value': False, 'Confidence': 98.48948669433594},\n",
       "    'Sunglasses': {'Value': False, 'Confidence': 99.64108276367188},\n",
       "    'Gender': {'Value': 'Female', 'Confidence': 99.82144165039062},\n",
       "    'Beard': {'Value': False, 'Confidence': 99.4758529663086},\n",
       "    'Mustache': {'Value': False, 'Confidence': 99.84904479980469},\n",
       "    'EyesOpen': {'Value': True, 'Confidence': 99.75717163085938},\n",
       "    'MouthOpen': {'Value': True, 'Confidence': 99.37398529052734},\n",
       "    'Emotions': [{'Type': 'HAPPY', 'Confidence': 99.4581298828125},\n",
       "     {'Type': 'SURPRISED', 'Confidence': 0.2348552942276001},\n",
       "     {'Type': 'CONFUSED', 'Confidence': 0.14081569015979767},\n",
       "     {'Type': 'CALM', 'Confidence': 0.08340548723936081},\n",
       "     {'Type': 'DISGUSTED', 'Confidence': 0.02932959981262684},\n",
       "     {'Type': 'ANGRY', 'Confidence': 0.02135021612048149},\n",
       "     {'Type': 'SAD', 'Confidence': 0.017474595457315445},\n",
       "     {'Type': 'FEAR', 'Confidence': 0.014648131094872952}],\n",
       "    'Landmarks': [{'Type': 'eyeLeft',\n",
       "      'X': 0.5008122324943542,\n",
       "      'Y': 0.2926607131958008},\n",
       "     {'Type': 'eyeRight', 'X': 0.5533066391944885, 'Y': 0.29391956329345703},\n",
       "     {'Type': 'mouthLeft', 'X': 0.50468510389328, 'Y': 0.37433600425720215},\n",
       "     {'Type': 'mouthRight', 'X': 0.548578143119812, 'Y': 0.3755023777484894},\n",
       "     {'Type': 'nose', 'X': 0.5259422659873962, 'Y': 0.33632034063339233},\n",
       "     {'Type': 'leftEyeBrowLeft',\n",
       "      'X': 0.4813968539237976,\n",
       "      'Y': 0.27380919456481934},\n",
       "     {'Type': 'leftEyeBrowRight',\n",
       "      'X': 0.49643805623054504,\n",
       "      'Y': 0.2640014588832855},\n",
       "     {'Type': 'leftEyeBrowUp',\n",
       "      'X': 0.5114665031433105,\n",
       "      'Y': 0.268490731716156},\n",
       "     {'Type': 'rightEyeBrowLeft',\n",
       "      'X': 0.5414578914642334,\n",
       "      'Y': 0.2691454291343689},\n",
       "     {'Type': 'rightEyeBrowRight',\n",
       "      'X': 0.5567986369132996,\n",
       "      'Y': 0.2652927339076996},\n",
       "     {'Type': 'rightEyeBrowUp',\n",
       "      'X': 0.5724166631698608,\n",
       "      'Y': 0.2757580578327179},\n",
       "     {'Type': 'leftEyeLeft', 'X': 0.491502583026886, 'Y': 0.29219508171081543},\n",
       "     {'Type': 'leftEyeRight',\n",
       "      'X': 0.5110752582550049,\n",
       "      'Y': 0.2937374711036682},\n",
       "     {'Type': 'leftEyeUp', 'X': 0.5005900263786316, 'Y': 0.2885282337665558},\n",
       "     {'Type': 'leftEyeDown',\n",
       "      'X': 0.5009245872497559,\n",
       "      'Y': 0.29633671045303345},\n",
       "     {'Type': 'rightEyeLeft', 'X': 0.5427493453025818, 'Y': 0.294473797082901},\n",
       "     {'Type': 'rightEyeRight',\n",
       "      'X': 0.5625624060630798,\n",
       "      'Y': 0.2938501536846161},\n",
       "     {'Type': 'rightEyeUp', 'X': 0.5532637238502502, 'Y': 0.28976523876190186},\n",
       "     {'Type': 'rightEyeDown',\n",
       "      'X': 0.5528813004493713,\n",
       "      'Y': 0.2975648045539856},\n",
       "     {'Type': 'noseLeft', 'X': 0.5163049697875977, 'Y': 0.34561052918434143},\n",
       "     {'Type': 'noseRight', 'X': 0.535671591758728, 'Y': 0.34603896737098694},\n",
       "     {'Type': 'mouthUp', 'X': 0.5258429050445557, 'Y': 0.36514967679977417},\n",
       "     {'Type': 'mouthDown', 'X': 0.5257482528686523, 'Y': 0.3899393677711487},\n",
       "     {'Type': 'leftPupil', 'X': 0.5008122324943542, 'Y': 0.2926607131958008},\n",
       "     {'Type': 'rightPupil', 'X': 0.5533066391944885, 'Y': 0.29391956329345703},\n",
       "     {'Type': 'upperJawlineLeft',\n",
       "      'X': 0.4705658555030823,\n",
       "      'Y': 0.29464781284332275},\n",
       "     {'Type': 'midJawlineLeft',\n",
       "      'X': 0.48082634806632996,\n",
       "      'Y': 0.3831453323364258},\n",
       "     {'Type': 'chinBottom', 'X': 0.525853157043457, 'Y': 0.4325529932975769},\n",
       "     {'Type': 'midJawlineRight',\n",
       "      'X': 0.5732346177101135,\n",
       "      'Y': 0.3849251866340637},\n",
       "     {'Type': 'upperJawlineRight',\n",
       "      'X': 0.5845797657966614,\n",
       "      'Y': 0.2969152331352234}],\n",
       "    'Pose': {'Roll': -0.19935601949691772,\n",
       "     'Yaw': -0.22715625166893005,\n",
       "     'Pitch': 5.689812660217285},\n",
       "    'Quality': {'Brightness': 83.48728942871094,\n",
       "     'Sharpness': 92.22801208496094},\n",
       "    'Confidence': 99.99779510498047}}],\n",
       " 'FaceModelVersion': '5.0',\n",
       " 'UnindexedFaces': [],\n",
       " 'ResponseMetadata': {'RequestId': '13f1ea04-e494-403c-9b1d-52d459318e92',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '13f1ea04-e494-403c-9b1d-52d459318e92',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3714',\n",
       "   'date': 'Tue, 19 Oct 2021 18:38:04 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show JSON response returned by Rekognition Labels API (Object Detection)\n",
    "# In the JSON response below, you will see Label, detected instances, confidence score and additional information.\n",
    "\n",
    "display(faceIndexResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FaceId': '50d567f5-a820-4ad7-b478-f835fb599ff5', 'BoundingBox': {'Width': 0.111285001039505, 'Height': 0.22814199328422546, 'Left': 0.4676940143108368, 'Top': 0.2081580013036728}, 'ImageId': '805a5bb9-6f62-3e3c-89a9-96fde1cde3c7', 'ExternalImageId': 'facecollection1.jpg', 'Confidence': 99.997802734375}\n",
      "faces count: 1\n"
     ]
    }
   ],
   "source": [
    "maxResults=2\n",
    "faces_count=0\n",
    "tokens=True\n",
    "\n",
    "response=rekognition.list_faces(CollectionId=collection_id,\n",
    "                                MaxResults=maxResults)\n",
    "while tokens:\n",
    "\n",
    "    faces=response['Faces']\n",
    "\n",
    "    for face in faces:\n",
    "        print (face)\n",
    "        faces_count+=1\n",
    "    if 'NextToken' in response:\n",
    "        nextToken=response['NextToken']\n",
    "        response=rekognition.list_faces(CollectionId=collection_id,\n",
    "                                    NextToken=nextToken,MaxResults=maxResults)\n",
    "    else:\n",
    "        tokens=False\n",
    "\n",
    "print(\"faces count: \" + str(faces_count))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize objects in video\n",
    " Object recognition in video is an async operation. \n",
    "https://docs.aws.amazon.com/rekognition/latest/dg/API_StartLabelDetection.html. \n",
    "\n",
    "- First we start a label detection job which returns a Job Id.\n",
    "- We can then call `get_label_detection` to get the job status and after job is complete, we can get object metadata.\n",
    "- In production use cases, you would usually use StepFunction or SNS topic to get notified when job is complete.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoName = \"media/object-detection/GrandTour720.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call Rekognition to start a job for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start video label recognition job\n",
    "startLabelDetection = rekognition.start_label_detection(\n",
    "    Video={\n",
    "        'S3Object': {\n",
    "            'Bucket': bucketName,\n",
    "            'Name': videoName,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "labelsJobId = startLabelDetection['JobId']\n",
    "display(\"Job Id: {0}\".format(labelsJobId))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional (Optional) Request Attributes\n",
    "\n",
    "ClientRequestTokenL\n",
    "https://docs.aws.amazon.com/rekognition/latest/dg/API_StartLabelDetection.html#rekognition-StartLabelDetection-request-ClientRequestToken\n",
    "\n",
    "JobTag:\n",
    "https://docs.aws.amazon.com/rekognition/latest/dg/API_StartLabelDetection.html#rekognition-StartLabelDetection-request-JobTag\n",
    "\n",
    "MinConfidence:\n",
    "https://docs.aws.amazon.com/rekognition/latest/dg/API_StartLabelDetection.html#rekognition-StartLabelDetection-request-MinConfidence\n",
    "\n",
    "NotificationChannel:\n",
    "https://docs.aws.amazon.com/rekognition/latest/dg/API_StartLabelDetection.html#rekognition-StartLabelDetection-request-NotificationChannel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for object detection job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for object detection job to complete\n",
    "# In production use cases, you would usually use StepFunction or SNS topic to get notified when job is complete.\n",
    "getObjectDetection = rekognition.get_label_detection(\n",
    "    JobId=labelsJobId,\n",
    "    SortBy='TIMESTAMP'\n",
    ")\n",
    "\n",
    "while(getObjectDetection['JobStatus'] == 'IN_PROGRESS'):\n",
    "    time.sleep(5)\n",
    "    print('.', end='')\n",
    " \n",
    "    getObjectDetection = rekognition.get_label_detection(\n",
    "    JobId=labelsJobId,\n",
    "    SortBy='TIMESTAMP')\n",
    "    \n",
    "display(getObjectDetection['JobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review raw JSON reponse from Rekognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show JSON response returned by Rekognition Object Detection API\n",
    "# In the JSON response below, you will see list of detected objects and activities.\n",
    "# For each detected object, you will see information like Timestamp\n",
    "\n",
    "display(getObjectDetection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display names of recognized objects in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flaggedObjectsInVideo = [\"Car\"]\n",
    "\n",
    "theObjects = {}\n",
    "\n",
    "# Display timestamps and objects detected at that time\n",
    "strDetail = \"Objects detected in video<br>=======================================<br>\"\n",
    "strOverall = \"Objects in the overall video:<br>=======================================<br>\"\n",
    "\n",
    "# Objects detected in each frame\n",
    "for obj in getObjectDetection['Labels']:\n",
    "    ts = obj [\"Timestamp\"]\n",
    "    cconfidence = obj['Label'][\"Confidence\"]\n",
    "    oname = obj['Label'][\"Name\"]\n",
    "    \n",
    "    if(oname in flaggedObjectsInVideo):\n",
    "        print(\"Found flagged object at {} ms: {} (Confidence: {})\".format(ts, oname, round(cconfidence,2)))\n",
    "    \n",
    "    strDetail = strDetail + \"At {} ms: {} (Confidence: {})<br>\".format(ts, oname, round(cconfidence,2))\n",
    "    if oname in theObjects:\n",
    "        cojb = theObjects[oname]\n",
    "        theObjects[oname] = {\"Name\" : oname, \"Count\": 1+cojb[\"Count\"]}\n",
    "    else:\n",
    "        theObjects[oname] = {\"Name\" : oname, \"Count\": 1}\n",
    "\n",
    "# Unique objects detected in video\n",
    "for theObject in theObjects:\n",
    "    strOverall = strOverall + \"Name: {}, Count: {}<br>\".format(theObject, theObjects[theObject][\"Count\"])\n",
    "\n",
    "# Display results\n",
    "display(HTML(strOverall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show video in the player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show video in a player\n",
    "\n",
    "s3VideoUrl = s3.generate_presigned_url('get_object', Params={'Bucket': bucketName, 'Key': videoName})\n",
    "\n",
    "videoTag = \"<video controls='controls' autoplay width='640' height='360' name='Video' src='{0}'></video>\".format(s3VideoUrl)\n",
    "\n",
    "videoui = \"<table><tr><td style='vertical-align: top'>{}</td></tr></table>\".format(videoTag)\n",
    "\n",
    "display(HTML(videoui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listui = \"<table><tr><td style='vertical-align: top'>{}</td></tr></table>\".format(strDetail)\n",
    "display(HTML(listui))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker Safety with Amazon Rekognition\n",
    "***\n",
    "You can use Amazon Rekognition to detect if certain objects are not present in the image or video. For example you can perform worker safety audit by revieweing images/video of a construction site  and detecting if there are any workers without safety hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageName = \"media/object-detection/hat-detection.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(IImage(url=s3.generate_presigned_url('get_object', Params={'Bucket': bucketName, 'Key': imageName})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Amazon Rekognition to detect objects in the image\n",
    "\n",
    "detectLabelsResponse = rekognition.detect_labels(\n",
    "    Image={\n",
    "        'S3Object': {\n",
    "            'Bucket': bucketName,\n",
    "            'Name': imageName,\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Rekognition response\n",
    "display(detectLabelsResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will display image with bounded boxes around recognized objects\n",
    "# We will call this function in next step\n",
    "  \n",
    "def drawBoundingBoxes (sourceImage, boxes):\n",
    "    # blue, green, red, grey\n",
    "    colors = ((255,255,255),(255,255,255),(76,182,252),(52,194,123))\n",
    "    \n",
    "    # Download image locally\n",
    "    imageLocation = tempFolder+os.path.basename(sourceImage)\n",
    "    s3.download_file(bucketName, sourceImage, imageLocation)\n",
    "\n",
    "    # Draws BB on Image\n",
    "    bbImage = Image.open(imageLocation)\n",
    "    draw = ImageDraw.Draw(bbImage)\n",
    "    width, height = bbImage.size\n",
    "    col = 0\n",
    "    maxcol = len(colors)\n",
    "    line= 3\n",
    "    for box in boxes:\n",
    "        x1 = int(box[1]['Left'] * width)\n",
    "        y1 = int(box[1]['Top'] * height)\n",
    "        x2 = int(box[1]['Left'] * width + box[1]['Width'] * width)\n",
    "        y2 = int(box[1]['Top'] * height + box[1]['Height']  * height)\n",
    "        \n",
    "        draw.text((x1,y1),box[0],colors[col])\n",
    "        for l in range(line):\n",
    "            draw.rectangle((x1-l,y1-l,x2+l,y2+l),outline=colors[col])\n",
    "        col = (col+1)%maxcol\n",
    "    \n",
    "    imageFormat = \"PNG\"\n",
    "    ext = sourceImage.lower()\n",
    "    if(ext.endswith('jpg') or ext.endswith('jpeg')):\n",
    "        imageFormat = 'JPEG'\n",
    "\n",
    "    bbImage.save(imageLocation,format=imageFormat)\n",
    "\n",
    "    display(bbImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image and bounded boxes around detected objects\n",
    "boxes = []\n",
    "objects = detectLabelsResponse['Labels']\n",
    "for obj in objects:\n",
    "    for einstance in obj[\"Instances\"]:\n",
    "        boxes.append ((obj['Name'], einstance['BoundingBox']))\n",
    "    \n",
    "drawBoundingBoxes(imageName, boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchPersonsAndHats(personsList, hardhatsList):\n",
    "\n",
    "    persons = []\n",
    "    hardhats = []\n",
    "    personsWithHats = []\n",
    "\n",
    "    for person in personsList:\n",
    "        persons.append(person)\n",
    "    for hardhat in hardhatsList:\n",
    "        hardhats.append(hardhat)\n",
    "\n",
    "    h = 0\n",
    "    matched = 0\n",
    "    totalHats = len(hardhats)\n",
    "    while(h < totalHats):\n",
    "        hardhat = hardhats[h-matched]\n",
    "        totalPersons = len(persons)\n",
    "        p = 0\n",
    "        while(p < totalPersons):\n",
    "            person = persons[p]\n",
    "            if(not (hardhat['BoundingBoxCoordinates']['x2'] < person['BoundingBoxCoordinates']['x1']\n",
    "                or hardhat['BoundingBoxCoordinates']['x1'] > person['BoundingBoxCoordinates']['x2']\n",
    "                or hardhat['BoundingBoxCoordinates']['y4'] < person['BoundingBoxCoordinates']['y1']\n",
    "                    or hardhat['BoundingBoxCoordinates']['y1'] > person['BoundingBoxCoordinates']['y4']\n",
    "                )):\n",
    "\n",
    "                personsWithHats.append({'Person' : person, 'Hardhat' : hardhat})\n",
    "\n",
    "                del persons[p]\n",
    "                del hardhats[h - matched]\n",
    "\n",
    "                matched = matched + 1\n",
    "\n",
    "                break\n",
    "            p = p + 1\n",
    "        h = h + 1\n",
    "\n",
    "    return (personsWithHats, persons, hardhats)\n",
    "\n",
    "def getBoundingBoxCoordinates(boundingBox, imageWidth, imageHeight):\n",
    "    x1 = 0\n",
    "    y1 = 0\n",
    "    x2 = 0\n",
    "    y2 = 0\n",
    "    x3 = 0\n",
    "    y3 = 0\n",
    "    x4 = 0\n",
    "    y4 = 0\n",
    "\n",
    "    boxWidth = boundingBox['Width']*imageWidth\n",
    "    boxHeight = boundingBox['Height']*imageHeight\n",
    "\n",
    "    x1 = boundingBox['Left']*imageWidth\n",
    "    y1 = boundingBox['Top']*imageWidth\n",
    "\n",
    "    x2 = x1 + boxWidth\n",
    "    y2 = y1\n",
    "\n",
    "    x3 = x2\n",
    "    y3 = y1 + boxHeight\n",
    "\n",
    "    x4 = x1\n",
    "    y4 = y3\n",
    "\n",
    "    return({'x1': x1, 'y1' : y1, 'x2' : x2, 'y2' : y2, 'x3' : x3, 'y3' : y3, 'x4' : x4, 'y4' : y4})\n",
    "\n",
    "def getPersonsAndHardhats(labelsResponse, imageWidth, imageHeight):\n",
    "\n",
    "    persons = []\n",
    "    hardhats = []\n",
    "\n",
    "    for label in labelsResponse['Labels']:\n",
    "        if label['Name'] == 'Person' and 'Instances' in label:\n",
    "            for person in label['Instances']:\n",
    "                    persons.append({'BoundingBox' : person['BoundingBox'], 'BoundingBoxCoordinates' : getBoundingBoxCoordinates(person['BoundingBox'], imageWidth, imageHeight), 'Confidence' : person['Confidence']})\n",
    "        elif ((label['Name'] == 'Hardhat' or label['Name'] == 'Helmet') and 'Instances' in label):\n",
    "            for hardhat in label['Instances']:\n",
    "                hardhats.append({'BoundingBox' : hardhat['BoundingBox'], 'BoundingBoxCoordinates' : getBoundingBoxCoordinates(hardhat['BoundingBox'], imageWidth, imageHeight), 'Confidence' : hardhat['Confidence']})\n",
    "\n",
    "    return (persons, hardhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3Resource = boto3.resource('s3')\n",
    "bucket = s3Resource.Bucket(bucketName)\n",
    "iojb = bucket.Object(imageName)\n",
    "response = iojb.get()\n",
    "file_stream = response['Body']\n",
    "im = Image.open(file_stream)\n",
    "imageWidth, imageHeight = im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons, hardhats = getPersonsAndHardhats(detectLabelsResponse, imageWidth, imageHeight)\n",
    "\n",
    "personsWithHats, personsWithoutHats, hatsWihoutPerson = matchPersonsAndHats(persons, hardhats)\n",
    "\n",
    "personsWithHatsCount = len(personsWithHats)\n",
    "personsWithoutHatsCount = len(personsWithoutHats)\n",
    "hatsWihoutPersonCount = len(hatsWihoutPerson)\n",
    "\n",
    "outputMessage = \"Person(s): {}\".format(personsWithHatsCount+personsWithoutHatsCount)\n",
    "outputMessage = outputMessage + \"\\nPerson(s) With Safety Hat: {}\\nPerson(s) Without Safety Hat: {}\".format(personsWithHatsCount, personsWithoutHatsCount)\n",
    "print(outputMessage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn more about building an end to end worker safety app\n",
    "To learn on how to build an end to end worker safety app, learn more at https://github.com/aws-samples/aws-deeplens-worker-safety-project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### References\n",
    "- https://docs.aws.amazon.com/rekognition/latest/dg/API_DetectLabels.html\n",
    "- https://docs.aws.amazon.com/rekognition/latest/dg/API_StartLabelDetection.html\n",
    "- https://docs.aws.amazon.com/rekognition/latest/dg/API_GetLabelDetection.html\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully used Amazon Rekognition to identify specific objects in images and videos. In the next module,nyou will learn how to recognize explicit, suggestive or violent content in the images and videos."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13e3d802fcb982db01ceb47e9c95427b0f1a0d746872494b489b2e1d16922009"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
